---
title: "Homework2PSTAT231"
author: "Evan Ji"
date: "2022-10-06"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(tidyverse)
library(tidymodels)
library(tinytex)
```
#Homework 2
##Question 1

Your goal is to predict abalone age, which is calculated as the number of rings plus 1.5. Notice there currently is no age variable in the data set. Add age to the data set.

Assess and describe the distribution of age.

```{r}
abalone <- read.csv("C:/Users/Evan Ji/Downloads/PSTAT231/homework-2/homework-2/data/abalone.csv")
abalone$age <- abalone$rings +1.5
hist(abalone$age)
```
The distibution of age seem to be right skewed with a mean of around 11.

##Question 2

Split the abalone data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data.

Remember that you’ll need to set a seed at the beginning of the document to reproduce your results

```{r}
set.seed(100)
data_split <- initial_split(abalone, prop = 1/2,strata=type)
train_data <- training(data_split)
test_data  <- testing(data_split)
```

##Question 3

Using the training data, create a recipe predicting the outcome variable, age, with all other predictor variables. Note that you should not include rings to predict age. Explain why you shouldn’t use rings to predict age.

```{r}
abalonerecipe <- recipe(age ~., data=train_data) %>% update_role(rings,new_role ='ignore') %>% step_dummy(all_nominal_predictors()) %>% step_interact(terms = ~ diameter:longest_shell + shell_weight:shucked_weight)
summary(abalonerecipe)
```
**Note: Type:Shucked_weight interaction was taken out as it resulted in error
Rings should not be used to predict age, as age is a linear function of rings and thus would defeat the purpose of trying to predict the age.

##Question 4

Create and store a linear regression object using the "lm" engine.
```{r}
abalonelr_mod <- linear_reg() %>% set_engine("lm")
```

##Question 5
```{r}
abaloneworkflow <- workflow() %>% add_model(abalonelr_mod) %>% add_recipe(abalonerecipe)
abaloneworkflow
```

##Question 6

Use your fit() object to predict the age of a hypothetical female abalone with longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, shell_weight = 1.

*Something is wrong with my fit function when it has type, so my model will not have type in it.
```{r}
abalonefit <- fit(abaloneworkflow,train_data)
abalonefit %>% extract_fit_parsnip() %>% tidy()
dfabalone <- data.frame(longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, shell_weight = 1, type = 'F', rings = 1)
dfabalone
predict(abalonefit,dfabalone)
```
##Question 7 
Now you want to assess your model’s performance. To do this, use the yardstick package:

```{r}
abalone_train_res <- predict(abalonefit,new_data=train_data)
abalone_train_res <- bind_cols(abalone_train_res,new_data=train_data)
abalone_train_res %>% 
  ggplot(aes(x = .pred, y = age)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()
rmse(abalone_train_res, truth = age, estimate = .pred)
abalone_metrics <- metric_set(rmse, rsq, mae)
abalone_metrics(abalone_train_res, truth = age, 
                estimate = .pred)
```
With a R^2 value of 0.577 this means that 57.7% of the variance in the response/outcome variable can be explained by the predictor variables. This is not very good, and shows that our model is not a very good one.

#Required for 231 Students
##Question 8
$$E[(y_0−\hat f(x_0))^2]=Var(\hat f(x0))+[Bias(\hat f(x_0))]^2+Var(ϵ)$$
Which term(s) in the bias-variance tradeoff above represent the reproducible error? Which term(s) represent the irreducible error?
$Var(\hat f(x0))+[Bias(\hat f(x_0))]^2$ represents the reproducible error. Var(ϵ) represents the irreducible error.

##Question 9
Using the bias-variance tradeoff above, demonstrate that the expected test error is always at least as large as the irreducible error.

If we minimize the reproducible error, that is $Var(\hat f(x0))+[Bias(\hat f(x_0))]^2 = 0$ then the bias-variance tradeoff will be $E[(y_0−\hat f(x_0))^2]=0+0+Var(ϵ)$. This shows that the expected test error is always as large as the irreducible error.

##Question 10 
Prove the bias-variance tradeoff.

$E[(y_0−\hat f(x_0))^2] = E[(f(x) + E[\hat f(x)] - E[\hat f (x)] - \hat f(x)) ^2 ]$
$=E[(E|\hat f(x) -f(x))^2] + E[( \hat f(x) - E[ \hat f(x)]^2] - 2E[(f(x) - E[ \hat f(x)])](\hat f (x) - E [\hat f (x)]) = (E [\hat f(x) - f (x))^2 + E[(\hat f(x) - E[\hat f(x)])]^2 -2E[f(x) - E[\hat f(x)] - E[\hat f (x)])= bias[\hat f (x)] + variance[\hat f(x)]$





